{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model context protocol (mcp) - complete guide\n",
    "\n",
    "this notebook demonstrates the full mcp ecosystem:\n",
    "1. connecting to docker mcp gateway\n",
    "2. creating custom mcp servers with tools, resources, and prompts\n",
    "3. using mcp inspector for debugging\n",
    "4. implementing mcp clients\n",
    "5. building multi-agent systems with mcp tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages (run once)\n",
    "# !pip install \"mcp[cli]\" llama-index llama-index-tools-mcp llama-index-llms-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from llama_index.tools.mcp import BasicMCPClient, aget_tools_from_mcp_url\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent.workflow import ReActAgent, AgentWorkflow, AgentStream, ToolCallResult\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DOCKER_GATEWAY_TOKEN = os.getenv(\"DOCKER_GATEWAY_TOKEN\", \"\")  # copy from docker gateway output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. docker mcp gateway - connecting to remote mcp servers\n",
    "\n",
    "### what is docker mcp gateway?\n",
    "docker mcp gateway allows you to connect multiple mcp servers through docker desktop and access them via a single endpoint.\n",
    "\n",
    "### setup steps:\n",
    "1. install go (required for mcp gateway)\n",
    "2. open docker desktop\n",
    "3. go to settings → enable \"docker mcp toolkit\"\n",
    "4. navigate to mcp toolkit → clients (you'll see cursor, codex, claude desktop)\n",
    "5. start the docker mcp gateway:\n",
    "   ```bash\n",
    "   docker mcp gateway run --port 8080 --transport streaming\n",
    "   ```\n",
    "6. copy the bearer token from the output and add to .env file\n",
    "\n",
    "### gateway urls:\n",
    "- **from docker containers**: `http://host.docker.internal:8080/mcp`\n",
    "- **from local machine**: `http://localhost:8080/mcp`\n",
    "\n",
    "### important: authentication\n",
    "the docker gateway requires bearer token authentication. make sure to:\n",
    "1. copy the token from the gateway startup output\n",
    "2. add it to your `.env` file as `DOCKER_GATEWAY_TOKEN=your_token_here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded: n3jihrmridkv4jdh7jwg...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "DOCKER_GATEWAY_TOKEN = os.getenv(\"DOCKER_GATEWAY_TOKEN\", \"\")\n",
    "print(f\"Reloaded: {DOCKER_GATEWAY_TOKEN[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== available tools from docker gateway ===\n",
      "found 9 tools:\n",
      "  - code-mode: Create a JavaScript-enabled tool that combines multiple MCP server tools. \n",
      "This allows you to write scripts that call multiple tools and combine their results.\n",
      "Use the mcp-find tool to find servers and make sure they are are ready with the mcp-add tool. When running\n",
      "mcp-add, we don't have to activate the tools.\n",
      "\n",
      "  - get_timed_transcript: Retrieves the transcript of a YouTube video with timestamps.\n",
      "  - get_transcript: Retrieves the transcript of a YouTube video.\n",
      "  - get_video_info: Retrieves the video information.\n",
      "  - mcp-add: Add a new MCP server to the session. \n",
      "The server must exist in the catalog.\n",
      "  - mcp-config-set: Set configuration for an MCP server. \n",
      "The config object will be validated against the server's config schema. If validation fails, the error message will include the correct schema.\n",
      "  - mcp-exec: Execute a tool that exists in the current session. This allows calling tools that may not be visible in listTools results.\n",
      "  - mcp-find: Find MCP servers in the current catalog by name, title, or description.\n",
      "If the user is looking for new capabilities, use this tool to search the MCP catalog for servers that should potentially be enabled.\n",
      "This will not enable the server but will return information about servers that could be enabled.\n",
      "If we find an mcp server, it can be added with the mcp-add tool, and configured with mcp-config-set.\n",
      "  - mcp-remove: Remove an MCP server from the registry and reload the configuration. This will disable the server.\n"
     ]
    }
   ],
   "source": [
    "# connect to docker mcp gateway\n",
    "async def explore_docker_gateway():\n",
    "    \"\"\"explore mcp servers available through docker gateway\"\"\"\n",
    "    \n",
    "    if not DOCKER_GATEWAY_TOKEN:\n",
    "        print(\"⚠️ warning: DOCKER_GATEWAY_TOKEN not set in .env file\")\n",
    "        print(\"   copy token from docker gateway output: 'Use Bearer token: Authorization: Bearer <token>'\")\n",
    "        return None\n",
    "    \n",
    "    # create client with authentication\n",
    "    docker_client = BasicMCPClient(\n",
    "        \"http://localhost:8080/mcp\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {DOCKER_GATEWAY_TOKEN}\",\n",
    "            \"Accept\": \"application/json, text/event-stream\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # list available tools from all connected mcp servers\n",
    "    print(\"=== available tools from docker gateway ===\")\n",
    "    try:\n",
    "        available_tools = await docker_client.list_tools()\n",
    "        print(f\"found {len(available_tools.tools)} tools:\")\n",
    "        for tool in available_tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error connecting to docker gateway: {e}\")\n",
    "        import traceback\n",
    "        print(\"\\nfull error details:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nmake sure:\")\n",
    "        print(\"  1. docker gateway is running: docker mcp gateway run --port 8080 --transport streaming\")\n",
    "        print(\"  2. DOCKER_GATEWAY_TOKEN is set in .env file\")\n",
    "        return None\n",
    "    \n",
    "    return docker_client\n",
    "\n",
    "# run the exploration\n",
    "docker_client = await explore_docker_gateway()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: youtube transcription with docker gateway\n",
    "\n",
    "this example uses an mcp server for youtube transcription connected through docker gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 tools from docker gateway\n",
      "\n",
      "=== transcribing youtube video ===\n",
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "\n",
      "response: Here is the transcript of the video \"Building an MCP server in 2 minutes\":\n",
      "\n",
      "\"Ever wish AI could connect to your tools like USBC connects to everything? That's what MCP, the model context protocol does. It securely links language models to your files, APIs, and tools, giving them real context, not just chat. Tools like Claude already use it. Here's how it works. Your LLM app like Claude or an IDE connects to multiple lightweight MCP servers, each linking to a data source like files, databases, or APIs. The MCP client talks to them all using one standard protocol. So adding new sources or switching LLMs is plug-and-play. \n",
      "\n",
      "Let's do something fun. We'll fetch live Pokémon data using MCP. Just make sure NodeJS is installed. First things first, we need to set up the project using these commands. We're using UV here, but you can also use pip. Okay, we're in VS Code and our project structure looks like this. Now, this Python file contains our MCP server logic. You can see that we imported the required modules and then created a server. We're fetching data from this API. And here we have a helper function that fetches the details of the Pokémon. \n",
      "\n",
      "Now, here we registered a tool to fetch the info. They're basically functions through which LLMs can interact with external systems, perform computations, and take actions in the real world. We can optionally provide a name and description for a tool as well. Like this, we have a tool to generate a fighting squad and another one to list the Pokémon. And at the end, we're running the server and we use the standard input-output transport to communicate between client and server. \n",
      "\n",
      "That's it. Our server is completed. Now I am going to CD into the directory and activate the environment. Then I'm going to start the development server. And this is how the UI looks. Connect to the server and we can see resources, prompts, and tools. In here we can list our tools and test them by clicking on the tool and then clicking on the run tool button on the right side. \n",
      "\n",
      "Now after terminating this operation, I'm repeating the same step, but this time I am installing the server into the client which in this case is Claude for desktop. This will change the Claude desktop config file, but you can manually edit the config file too by following these steps. Now we're in Claude and after reloading it, we can see three tools that we created. And if we look at our server, yeah, it's running. Now we can give a prompt like this and Claude will provide info based on the context we provided. \n",
      "\n",
      "Now let's give another prompt for creating a squad for a tournament and again it'll answer based on the context we provided. So, that's how you can create an MCP server and numerous tools to get answers based on the context that you only provide, either by local files or from databases. And I linked the source code in the description.\"\n"
     ]
    }
   ],
   "source": [
    "async def youtube_transcription_example():\n",
    "    \"\"\"example: transcribe a youtube video using docker gateway mcp\"\"\"\n",
    "    \n",
    "    if not DOCKER_GATEWAY_TOKEN:\n",
    "        print(\"⚠️ error: DOCKER_GATEWAY_TOKEN not set. please add it to your .env file\")\n",
    "        return\n",
    "    \n",
    "    # get transcription tools from docker gateway\n",
    "    docker_client = BasicMCPClient(\n",
    "        \"http://localhost:8080/mcp\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {DOCKER_GATEWAY_TOKEN}\",\n",
    "            \"Accept\": \"application/json, text/event-stream\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        tools = await aget_tools_from_mcp_url(\n",
    "            \"http://localhost:8080/mcp\",\n",
    "            client=docker_client,\n",
    "            allowed_tools=[\"get_transcript\"],  # specify which tools to use\n",
    "        )\n",
    "        \n",
    "        print(f\"loaded {len(tools)} tools from docker gateway\")\n",
    "        \n",
    "        # create an agent with transcription tools\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "        agent = ReActAgent(tools=tools, llm=llm, verbose=True)\n",
    "        \n",
    "        # transcribe a video\n",
    "        print(\"\\n=== transcribing youtube video ===\")\n",
    "        response = await agent.run(\n",
    "            user_msg=\"get me a transcript of the video at https://www.youtube.com/watch?v=Fhy_VFMlE9s\"\n",
    "        )\n",
    "        print(f\"\\nresponse: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "# run the example\n",
    "await youtube_transcription_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. creating a custom mcp server\n",
    "\n",
    "### what is an mcp server?\n",
    "an mcp server exposes three main components:\n",
    "1. **tools**: functions that ai agents can call\n",
    "2. **resources**: static or dynamic content (files, guides, documentation)\n",
    "3. **prompts**: pre-defined prompt templates with parameters\n",
    "\n",
    "### example: italian recipes mcp server\n",
    "\n",
    "let's examine our custom mcp server that provides italian recipe information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mcp server code structure\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# create an mcp server\n",
    "mcp = FastMCP(\"Local MCP Server - Recipes Assistant\")\n",
    "\n",
    "# 1. TOOLS - functions that agents can call\n",
    "@mcp.tool()\n",
    "def list_recipes() -> list[str]:\n",
    "    \"\"\"list all available italian recipes\"\"\"\n",
    "    return [\"Lasagna\", \"Pasta Carbonara\", \"Pizza Margherita\", ...]\n",
    "\n",
    "@mcp.tool()\n",
    "def get_recipe_instructions(recipe_name: str) -> str:\n",
    "    \"\"\"get instructions for preparing a specific italian recipe\"\"\"\n",
    "    # load from recipes.json and return formatted recipe\n",
    "    ...\n",
    "\n",
    "# 2. RESOURCES - static content accessible via uri\n",
    "@mcp.resource(\"guide://usage\")\n",
    "def get_usage_guide() -> str:\n",
    "    \"\"\"get instructions on how to use this mcp server\"\"\"\n",
    "    return \"\"\"# italian recipes mcp server - usage guide...\"\"\"\n",
    "\n",
    "# 3. PROMPTS - pre-defined prompt templates\n",
    "@mcp.prompt()\n",
    "def suggest_recipe(occasion: str = \"dinner\", dietary_preference: str = \"none\") -> str:\n",
    "    \"\"\"generate a prompt to get italian recipe suggestions\"\"\"\n",
    "    return f\"suggest an italian recipe perfect for {occasion}...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting the local mcp server\n",
    "\n",
    "to start your local mcp server, run in a terminal:\n",
    "\n",
    "```bash\n",
    "mcp run mcp_server/mcp_server.py --transport=streamable-http\n",
    "```\n",
    "\n",
    "this starts the server at `http://localhost:8000/mcp`\n",
    "\n",
    "**transport options:**\n",
    "- `streamable-http`: http streaming (recommended for remote access)\n",
    "- `stdio`: standard input/output (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mcp inspector - debugging your mcp server\n",
    "\n",
    "### what is mcp inspector?\n",
    "mcp inspector is a web-based debugging tool that lets you:\n",
    "- explore available tools, resources, and prompts\n",
    "- test tool calls with different parameters\n",
    "- view tool responses in real-time\n",
    "- debug your mcp server before integrating with clients\n",
    "\n",
    "### starting the inspector\n",
    "\n",
    "```bash\n",
    "# start the inspector (opens in browser)\n",
    "mcp dev ./mcp_server/mcp_server.py\n",
    "```\n",
    "\n",
    "### connecting to your server in inspector\n",
    "\n",
    "1. inspector opens at `http://localhost:5173`\n",
    "2. in another terminal, start your mcp server:\n",
    "   ```bash\n",
    "   mcp run mcp_server/mcp_server.py --transport=streamable-http\n",
    "   ```\n",
    "3. in the inspector:\n",
    "   - connection type: **proxy** (not direct)\n",
    "   - server url: `http://localhost:8000/mcp`\n",
    "4. click \"connect\"\n",
    "\n",
    "### what you can do in inspector:\n",
    "- **tools tab**: see all available tools, test them with parameters\n",
    "- **resources tab**: browse available resources (e.g., `guide://usage`)\n",
    "- **prompts tab**: test prompt templates with different arguments\n",
    "- **logs tab**: view server logs and debug issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. connecting to local mcp server from python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== exploring local mcp server ===\n",
      "\n",
      "--- tools ---\n",
      "  - list_recipes: list all available italian recipes\n",
      "  - get_recipe_instructions: get instructions for preparing a specific italian recipe\n",
      "\n",
      "--- resources ---\n",
      "  - guide://usage: get_usage_guide\n",
      "    description: get instructions on how to use this mcp server\n",
      "\n",
      "--- prompts ---\n",
      "  - suggest_recipe: generate a prompt to get italian recipe suggestions based on occasion and dietary preferences\n",
      "    arguments: ['occasion', 'dietary_preference']\n"
     ]
    }
   ],
   "source": [
    "async def explore_local_mcp_server():\n",
    "    \"\"\"explore our local italian recipes mcp server\"\"\"\n",
    "    \n",
    "    # connect to local mcp server\n",
    "    local_client = BasicMCPClient(\"http://localhost:8000/mcp\")\n",
    "    \n",
    "    print(\"=== exploring local mcp server ===\")\n",
    "    \n",
    "    try:\n",
    "        # list available tools\n",
    "        print(\"\\n--- tools ---\")\n",
    "        available_tools = await local_client.list_tools()\n",
    "        for tool in available_tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "        \n",
    "        # list available resources\n",
    "        print(\"\\n--- resources ---\")\n",
    "        try:\n",
    "            resources = await local_client.list_resources()\n",
    "            for resource in resources.resources:\n",
    "                print(f\"  - {resource.uri}: {resource.name}\")\n",
    "                if hasattr(resource, 'description') and resource.description:\n",
    "                    print(f\"    description: {resource.description}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  no resources available: {e}\")\n",
    "        \n",
    "        # list available prompts\n",
    "        print(\"\\n--- prompts ---\")\n",
    "        try:\n",
    "            prompts = await local_client.list_prompts()\n",
    "            for prompt in prompts.prompts:\n",
    "                print(f\"  - {prompt.name}: {prompt.description}\")\n",
    "                if hasattr(prompt, 'arguments') and prompt.arguments:\n",
    "                    print(f\"    arguments: {[arg.name for arg in prompt.arguments]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  no prompts available: {e}\")\n",
    "        \n",
    "        return local_client\n",
    "    except Exception as e:\n",
    "        print(f\"error connecting to local mcp server: {e}\")\n",
    "        print(\"make sure the local server is running:\")\n",
    "        print(\"  mcp run mcp_server/mcp_server.py --transport=streamable-http\")\n",
    "        return None\n",
    "\n",
    "# run the exploration\n",
    "local_client = await explore_local_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using local mcp server with an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2 tools from local mcp:\n",
      "  - list_recipes: list all available italian recipes\n",
      "  - get_recipe_instructions: get instructions for preparing a specific italian recipe\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: how to make pizza margherita?\n",
      "================================================================================\n",
      "\n",
      "[TOOL CALL #1]\n",
      "  Tool: list_recipes\n",
      "  Arguments: {}\n",
      "  Response: meta=None content=[TextContent(type='text', text='Lasagna', annotations=None), TextContent(type='text', text='Pasta Carbonara', annotations=None), TextContent(type='text', text='Pizza Margherita', ann...\n",
      "\n",
      "[TOOL CALL #2]\n",
      "  Tool: get_recipe_instructions\n",
      "  Arguments: {'recipe_name': 'Pizza Margherita'}\n",
      "  Response: meta=None content=[TextContent(type='text', text='**Pizza Margherita**\\n\\ningredients:\\n- pizza dough (500g)\\n- 400g san marzano tomatoes, crushed\\n- 300g fresh mozzarella, sliced\\n- fresh basil leave...\n",
      "\n",
      "================================================================================\n",
      "AGENT FINAL RESPONSE:\n",
      "================================================================================\n",
      "To make Pizza Margherita, follow these instructions:\n",
      "\n",
      "**Ingredients:**\n",
      "- 500g pizza dough\n",
      "- 400g San Marzano tomatoes, crushed\n",
      "- 300g fresh mozzarella, sliced\n",
      "- Fresh basil leaves\n",
      "- Extra virgin olive oil\n",
      "- Salt\n",
      "\n",
      "**Instructions:**\n",
      "1. Preheat the oven to maximum temperature (at least 475°F/250°C).\n",
      "2. Stretch the pizza dough into a round shape.\n",
      "3. Spread the crushed tomatoes evenly on the dough, leaving a border for the crust.\n",
      "4. Add salt to taste.\n",
      "5. Tear the mozzarella and distribute it evenly on the pizza.\n",
      "6. Drizzle with olive oil.\n",
      "7. Bake for 10-12 minutes until the crust is golden and the cheese is bubbly.\n",
      "8. Remove from the oven and top with fresh basil leaves.\n",
      "9. Drizzle with more olive oil and serve immediately.\n",
      "\n",
      "================================================================================\n",
      "Total tool calls made: 2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "async def local_mcp_agent_example():\n",
    "    \"\"\"create an agent that uses local mcp server tools\"\"\"\n",
    "    \n",
    "    # connect to local mcp\n",
    "    local_client = BasicMCPClient(\"http://localhost:8000/mcp\")\n",
    "    \n",
    "    try:\n",
    "        # get tools from local mcp\n",
    "        tools = await aget_tools_from_mcp_url(\n",
    "            \"http://localhost:8000/mcp\",\n",
    "            client=local_client,\n",
    "            allowed_tools=[\"list_recipes\", \"get_recipe_instructions\"],\n",
    "        )\n",
    "        \n",
    "        print(f\"loaded {len(tools)} tools from local mcp:\")\n",
    "        for tool in tools:\n",
    "            print(f\"  - {tool.metadata.name}: {tool.metadata.description}\")\n",
    "        \n",
    "        # create agent\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "        agent = ReActAgent(tools=tools, llm=llm, verbose=False)  # disable verbose to control output\n",
    "        \n",
    "        # test the agent\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"USER QUERY: how to make pizza margherita?\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # run with streaming to capture tool calls\n",
    "        handler = agent.run(user_msg=\"how to make pizza margherita?\")\n",
    "        \n",
    "        agent_response = \"\"\n",
    "        tool_call_count = 0\n",
    "        \n",
    "        async for ev in handler.stream_events():\n",
    "            if isinstance(ev, AgentStream):\n",
    "                # accumulate agent reasoning/response\n",
    "                agent_response += ev.delta\n",
    "            elif isinstance(ev, ToolCallResult):\n",
    "                tool_call_count += 1\n",
    "                print(f\"\\n[TOOL CALL #{tool_call_count}]\")\n",
    "                print(f\"  Tool: {ev.tool_name}\")\n",
    "                print(f\"  Arguments: {ev.tool_kwargs}\")\n",
    "                print(f\"  Response: {str(ev.tool_output)[:200]}...\")  # first 200 chars\n",
    "        \n",
    "        # get final response\n",
    "        final_response = await handler\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"AGENT FINAL RESPONSE:\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_response)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Total tool calls made: {tool_call_count}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        print(\"make sure the local mcp server is running\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# run the example\n",
    "await local_mcp_agent_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. remote mcp server hosted on fastmcp cloud\n",
    "\n",
    "### what is fastmcp cloud?\n",
    "fastmcp cloud is a hosting platform for mcp servers (similar to vercel for web apps).\n",
    "\n",
    "### setup steps:\n",
    "1. fork the template repository: https://github.com/PrefectHQ/fastmcp-quickstart-template\n",
    "2. visit https://fastmcp.cloud/\n",
    "3. sign in with github and select your forked repo\n",
    "4. configure security settings (public/private)\n",
    "5. deploy - you get a url like: `https://your-app.fastmcp.app/mcp`\n",
    "\n",
    "### benefits:\n",
    "- no need to host your own server\n",
    "- automatic deployments from github\n",
    "- free hosting for public mcp servers\n",
    "- easy integration with cursor, claude desktop, chatgpt, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== exploring remote fastmcp server ===\n",
      "found 2 tools:\n",
      "  - echo_tool: echo the input text\n",
      "  - get_weather: get weather information for a specific location using meteosource api\n",
      "\n",
      "args:\n",
      "    place_id: location identifier (e.g., 'london', 'new_york', 'tokyo')\n",
      "\n",
      "returns:\n",
      "    dict: weather information including current conditions and forecasts\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: what's the weather in rome?\n",
      "================================================================================\n",
      "\n",
      "[TOOL CALL #1]\n",
      "  Tool: get_weather\n",
      "  Arguments: {'place_id': 'rome'}\n",
      "  Response: meta=None content=[TextContent(type='text', text='{\"lat\":\"41.89193N\",\"lon\":\"12.51133E\",\"elevation\":20,\"timezone\":\"UTC\",\"units\":\"metric\",\"current\":{\"icon\":\"partly_sunny\",\"icon_num\":4,\"summary\":\"Partly sunny\",\"temperature\":13.8,\"wind\":{\"speed\":1.6,\"angle\":5,\"dir\":\"N\"},\"precipitation\":{\"total\":0.0,\"typ...\n",
      "\n",
      "================================================================================\n",
      "AGENT FINAL RESPONSE:\n",
      "================================================================================\n",
      "The current weather in Rome is partly sunny with a temperature of 13.8°C. The wind is coming from the north at a speed of 1.6 m/s, and there is no precipitation.\n",
      "\n",
      "================================================================================\n",
      "Total tool calls made: 1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "async def remote_mcp_example():\n",
    "    \"\"\"connect to remote mcp server hosted on fastmcp cloud\"\"\"\n",
    "    \n",
    "    # connect to remote fastmcp server (weather api)\n",
    "    remote_client = BasicMCPClient(\"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\")\n",
    "    \n",
    "    print(\"=== exploring remote fastmcp server ===\")\n",
    "    \n",
    "    try:\n",
    "        # list available tools\n",
    "        available_tools = await remote_client.list_tools()\n",
    "        print(f\"found {len(available_tools.tools)} tools:\")\n",
    "        for tool in available_tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "        \n",
    "        # get tools for agent\n",
    "        tools = await aget_tools_from_mcp_url(\n",
    "            \"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\",\n",
    "            client=remote_client,\n",
    "            allowed_tools=[\"get_weather\"],\n",
    "        )\n",
    "        \n",
    "        # create agent\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "        agent = ReActAgent(tools=tools, llm=llm, verbose=False)  # disable verbose to control output\n",
    "        \n",
    "        # test the agent with detailed logging\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"USER QUERY: what's the weather in rome?\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # run with streaming to capture tool calls\n",
    "        handler = agent.run(user_msg=\"what's the weather in rome?\")\n",
    "        \n",
    "        tool_call_count = 0\n",
    "        \n",
    "        async for ev in handler.stream_events():\n",
    "            if isinstance(ev, ToolCallResult):\n",
    "                tool_call_count += 1\n",
    "                print(f\"\\n[TOOL CALL #{tool_call_count}]\")\n",
    "                print(f\"  Tool: {ev.tool_name}\")\n",
    "                print(f\"  Arguments: {ev.tool_kwargs}\")\n",
    "                print(f\"  Response: {str(ev.tool_output)[:300]}...\")  # first 300 chars\n",
    "        \n",
    "        # get final response\n",
    "        final_response = await handler\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"AGENT FINAL RESPONSE:\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_response)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Total tool calls made: {tool_call_count}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# run the example\n",
    "await remote_mcp_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. hugging face spaces - 200+ ready-to-use mcp servers\n",
    "\n",
    "### what are hugging face mcp spaces?\n",
    "hugging face hosts over 200 mcp servers as gradio spaces, providing ready-to-use capabilities:\n",
    "- image generation (flux, stable diffusion)\n",
    "- text-to-speech\n",
    "- translation\n",
    "- and many more...\n",
    "\n",
    "### how to use:\n",
    "1. browse spaces at: https://huggingface.co/spaces\n",
    "2. look for spaces with mcp support\n",
    "3. use the gradio api endpoint: `https://[space-name].hf.space/gradio_api/mcp/`\n",
    "\n",
    "### example: flux image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found Hugging Face token: hf_BlICFoT...\n",
      "\n",
      "=== exploring hugging face flux mcp server ===\n",
      "found 2 tools:\n",
      "  - FLUX_1_schnell_get_seed: Determine and return the random seed to use for model generation. - MAX_SEED is the maximum value fo...\n",
      "  - FLUX_1_schnell_infer: Generate an image from a text prompt using the FLUX.1 [schnell] model. - Prompts must be in English....\n",
      "\n",
      "loaded 2 tools\n",
      "\n",
      "================================================================================\n",
      "USER QUERY: generate an image of a sunset over mountains\n",
      "================================================================================\n",
      "note: image generation may take 10-30 seconds...\n",
      "      attempting with authentication...\n",
      "\n",
      "\n",
      "[TOOL CALL #1]\n",
      "  Tool: FLUX_1_schnell_get_seed\n",
      "  Arguments: {'properties': AttributedDict([('randomize_seed', True), ('seed', None)])}\n",
      "  Response: [ERROR] meta=None content=[TextContent(type='text', text=\"Parameter `properties` is not a valid key-word argument. Please click on 'view API' in the footer of the Gradio app to see usage.\", annotations=None)]...\n",
      "\n",
      "[TOOL CALL #2]\n",
      "  Tool: FLUX_1_schnell_get_seed\n",
      "  Arguments: {'randomize_seed': True}\n",
      "  Response: [ERROR] meta=None content=[TextContent(type='text', text='1707062690', annotations=None)] isError=False...\n",
      "\n",
      "[TOOL CALL #3]\n",
      "  Tool: FLUX_1_schnell_infer\n",
      "  Arguments: {'prompt': 'a sunset over mountains', 'seed': 1707062690}\n",
      "  Response: [ERROR] meta=None content=[ImageContent(type='image', data='UklGRq6HAABXRUJQVlA4IKKHAABQlwOdASoABAAEPm02l0kkKSoqotUJYVANiWduc7G6ofJ6VPNqcbyCALAPyQWD4f1w0TC11P/5nig+m/5/pl/8zxMvD+or5vGc9/i4Wlrs/p78XoepX5ypd5D+...\n",
      "\n",
      "================================================================================\n",
      "AGENT FINAL RESPONSE:\n",
      "================================================================================\n",
      "![Sunset over Mountains](https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/file=/tmp/gradio/532b7080721bbf2acfa536855fb47d5de60ffc2c4b43114d92c48ddac625cd88/image.webp)\n",
      "\n",
      "================================================================================\n",
      "Total tool calls made: 3\n",
      "================================================================================\n",
      "\n",
      "⚠️ IMAGE GENERATION FAILED\n",
      "The Hugging Face Flux space encountered an error.\n",
      "This is common with free public spaces.\n"
     ]
    }
   ],
   "source": [
    "async def huggingface_mcp_example():\n",
    "    \"\"\"use hugging face gradio space for image generation\"\"\"\n",
    "    \n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    from PIL import Image\n",
    "    from IPython.display import display\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # connect to gradio flux mcp server\n",
    "    gradio_client = BasicMCPClient(\"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\")\n",
    "    \n",
    "    print(\"=== exploring hugging face flux mcp server ===\")\n",
    "    \n",
    "    # list available tools\n",
    "    try:\n",
    "        available_tools = await gradio_client.list_tools()\n",
    "        print(f\"found {len(available_tools.tools)} tools:\")\n",
    "        for tool in available_tools.tools:\n",
    "            print(f\"  - {tool.name}: {tool.description[:100]}...\")\n",
    "        \n",
    "        # get tools for agent\n",
    "        tools = await aget_tools_from_mcp_url(\n",
    "            \"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\",\n",
    "            client=gradio_client,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nloaded {len(tools)} tools\")\n",
    "        \n",
    "        # create agent\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "        agent = ReActAgent(tools=tools, llm=llm, verbose=False)\n",
    "        \n",
    "        # test the agent\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"USER QUERY: generate an image of a sunset over mountains\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"note: image generation may take 10-30 seconds...\\n\")\n",
    "        \n",
    "        handler = agent.run(user_msg=\"generate an image of a sunset over mountains\")\n",
    "        \n",
    "        tool_call_count = 0\n",
    "        generated_image = None\n",
    "        \n",
    "        async for ev in handler.stream_events():\n",
    "            if isinstance(ev, ToolCallResult):\n",
    "                tool_call_count += 1\n",
    "                print(f\"\\n[TOOL CALL #{tool_call_count}]\")\n",
    "                print(f\"  Tool: {ev.tool_name}\")\n",
    "                print(f\"  Arguments: {ev.tool_kwargs}\")\n",
    "                \n",
    "                output_str = str(ev.tool_output)\n",
    "                \n",
    "                # check if this is an ImageContent response\n",
    "                if 'ImageContent' in output_str and hasattr(ev.tool_output, 'content'):\n",
    "                    for content_item in ev.tool_output.content:\n",
    "                        if hasattr(content_item, 'type') and content_item.type == 'image':\n",
    "                            # decode and save image\n",
    "                            image_bytes = base64.b64decode(content_item.data)\n",
    "                            generated_image = Image.open(BytesIO(image_bytes))\n",
    "                            \n",
    "                            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                            image_path = f\"generated_image_{timestamp}.png\"\n",
    "                            generated_image.save(image_path)\n",
    "                            \n",
    "                            print(f\"  Response: [Image generated]\")\n",
    "                            print(f\"  Size: {generated_image.size}\")\n",
    "                            print(f\"  Saved to: {os.path.abspath(image_path)}\")\n",
    "                            break\n",
    "                else:\n",
    "                    print(f\"  Response: {output_str[:200]}...\")\n",
    "        \n",
    "        final_response = await handler\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL RESPONSE:\")\n",
    "        print(\"=\"*80)\n",
    "        print(final_response)\n",
    "        print(f\"\\nTotal tool calls: {tool_call_count}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # display the image\n",
    "        if generated_image:\n",
    "            print(\"\\nGENERATED IMAGE:\")\n",
    "            display(generated_image)\n",
    "        else:\n",
    "            print(\"\\n⚠️ No image generated (quota or API compatibility issues)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nerror: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# run the example\n",
    "await huggingface_mcp_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. hybrid agent - combining multiple mcp servers\n",
    "\n",
    "### the power of mcp: unified tool access\n",
    "\n",
    "one of the key benefits of mcp is the ability to combine tools from multiple sources:\n",
    "- local mcp servers (recipes)\n",
    "- remote mcp servers (weather)\n",
    "- docker gateway mcp servers (youtube transcription)\n",
    "- hugging face spaces (image generation)\n",
    "\n",
    "all in a single agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== creating hybrid agent with multiple mcp sources ===\n",
      "\n",
      "loading tools...\n",
      "  docker gateway: ✅ 1 tools\n",
      "  local mcp: ✅ 2 tools\n",
      "  remote fastmcp: ✅ 1 tools\n",
      "  hugging face: ✅ 2 tools\n",
      "\n",
      "================================================================================\n",
      "TOTAL TOOLS: 6 (docker: 1, local: 2, remote: 1, hf: 2)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TEST 1: RECIPE QUERY (Local MCP)\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Query: how to make tiramisu?\n",
      "\n",
      "→ Tool Call 1: list_recipes\n",
      "  Output: meta=None content=[TextContent(type='text', text='Lasagna', annotations=None), TextContent(type='text', text='Pasta Carbonara', annotations=None), Tex...\n",
      "→ Tool Call 2: get_recipe_instructions\n",
      "  Output: meta=None content=[TextContent(type='text', text=\"**Tiramisu**\\n\\ningredients:\\n- 6 egg yolks\\n- 150g sugar\\n- 500g mascarpone cheese\\n- 300ml strong ...\n",
      "\n",
      "✅ Success: Recipe retrieved\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TEST 2: WEATHER QUERY (Remote FastMCP)\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Query: what's the weather in milan?\n",
      "\n",
      "→ Tool Call 1: get_weather\n",
      "  Output: meta=None content=[TextContent(type='text', text='{\"lat\":\"45.46427N\",\"lon\":\"9.18951E\",\"elevation\":122,\"timezone\":\"UTC\",\"units\":\"metric\",\"current\":{\"ic...\n",
      "\n",
      "✅ Success: Weather data retrieved\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TEST 3: VIDEO TRANSCRIPTION (Docker Gateway)\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Query: get transcript from https://www.youtube.com/watch?v=Fhy_VFMlE9s\n",
      "\n",
      "→ Tool Call 1: get_transcript\n",
      "  Output: meta=None content=[TextContent(type='text', text='{\\n  \"title\": \"Building an MCP server in 2 minutes.... - YouTube\",\\n  \"transcript\": \"Ever wish AI co...\n",
      "\n",
      "✅ Success: Transcript retrieved (length: 2897 chars)\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TEST 4: IMAGE GENERATION (Hugging Face Space)\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "Query: generate an image of a sunset over mountains\n",
      "note: may fail due to quota limits\n",
      "\n",
      "→ Tool Call 1: FLUX_1_schnell_get_seed\n",
      "  Output: meta=None content=[TextContent(type='text', text='391200684', annotations=None)] isError=False...\n",
      "→ Tool Call 2: FLUX_1_schnell_get_seed\n",
      "  Output: meta=None content=[TextContent(type='text', text='2053573721', annotations=None)] isError=False...\n",
      "→ Tool Call 3: FLUX_1_schnell_infer\n",
      "  Output: meta=None content=[TextContent(type='text', text='Unlogged user is runnning out of daily ZeroGPU quotas. Signup for free on https://huggingface.co/joi...\n",
      "\n",
      "⚠️ Quota limit reached - ZeroGPU quota exceeded\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ HYBRID AGENT DEMO COMPLETE\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "✅ Connected to 4/4 MCP sources\n",
      "✅ Total tools available: 6\n",
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n"
     ]
    }
   ],
   "source": [
    "async def hybrid_agent_example():\n",
    "    \"\"\"create an agent with tools from multiple mcp sources\"\"\"\n",
    "    \n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    from PIL import Image\n",
    "    from IPython.display import display\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import sys\n",
    "    \n",
    "    print(\"=== creating hybrid agent with multiple mcp sources ===\")\n",
    "    \n",
    "    # connect to all mcp sources\n",
    "    if not DOCKER_GATEWAY_TOKEN:\n",
    "        print(\"⚠️ warning: DOCKER_GATEWAY_TOKEN not set. docker gateway tools will be skipped.\")\n",
    "    \n",
    "    docker_client = BasicMCPClient(\n",
    "        \"http://localhost:8080/mcp\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {DOCKER_GATEWAY_TOKEN}\",\n",
    "            \"Accept\": \"application/json, text/event-stream\"\n",
    "        }\n",
    "    ) if DOCKER_GATEWAY_TOKEN else None\n",
    "    remote_client = BasicMCPClient(\"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\")\n",
    "    local_client = BasicMCPClient(\"http://localhost:8000/mcp\")\n",
    "    gradio_client = BasicMCPClient(\"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\")\n",
    "    \n",
    "    # get tools from each source\n",
    "    print(\"\\nloading tools...\")\n",
    "    \n",
    "    docker_tools = []\n",
    "    if docker_client:\n",
    "        try:\n",
    "            docker_tools = await aget_tools_from_mcp_url(\n",
    "                \"http://localhost:8080/mcp\",\n",
    "                client=docker_client,\n",
    "                allowed_tools=[\"get_transcript\"],\n",
    "            )\n",
    "            print(f\"  docker gateway: ✅ {len(docker_tools)} tools\")\n",
    "        except Exception as e:\n",
    "            print(f\"  docker gateway: ❌ {str(e)[:50]}...\")\n",
    "    else:\n",
    "        print(\"  docker gateway: ⚠️ skipped (no token)\")\n",
    "    \n",
    "    try:\n",
    "        local_tools = await aget_tools_from_mcp_url(\n",
    "            \"http://localhost:8000/mcp\",\n",
    "            client=local_client,\n",
    "            allowed_tools=[\"list_recipes\", \"get_recipe_instructions\"],\n",
    "        )\n",
    "        print(f\"  local mcp: ✅ {len(local_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  local mcp: ❌ {str(e)[:50]}...\")\n",
    "        local_tools = []\n",
    "    \n",
    "    try:\n",
    "        remote_tools = await aget_tools_from_mcp_url(\n",
    "            \"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\",\n",
    "            client=remote_client,\n",
    "            allowed_tools=[\"get_weather\"],\n",
    "        )\n",
    "        print(f\"  remote fastmcp: ✅ {len(remote_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  remote fastmcp: ❌ {str(e)[:50]}...\")\n",
    "        remote_tools = []\n",
    "    \n",
    "    try:\n",
    "        gradio_tools = await aget_tools_from_mcp_url(\n",
    "            \"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\",\n",
    "            client=gradio_client,\n",
    "        )\n",
    "        print(f\"  hugging face: ✅ {len(gradio_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  hugging face: ❌ {str(e)[:50]}...\")\n",
    "        gradio_tools = []\n",
    "    \n",
    "    # combine all tools\n",
    "    all_tools = docker_tools + local_tools + remote_tools + gradio_tools\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOTAL TOOLS: {len(all_tools)} (docker: {len(docker_tools)}, local: {len(local_tools)}, remote: {len(remote_tools)}, hf: {len(gradio_tools)})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if not all_tools:\n",
    "        print(\"⚠️ no tools available. make sure at least one mcp server is running.\")\n",
    "        return\n",
    "    \n",
    "    # create hybrid agent\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "    agent = ReActAgent(tools=all_tools, llm=llm, verbose=False)\n",
    "    \n",
    "    # test 1: recipe query\n",
    "    if local_tools:\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"█ TEST 1: RECIPE QUERY (Local MCP)\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"\\nQuery: how to make tiramisu?\\n\")\n",
    "        \n",
    "        try:\n",
    "            handler1 = agent.run(user_msg=\"how to make tiramisu?\")\n",
    "            \n",
    "            # collect all events\n",
    "            tool_calls = []\n",
    "            async for ev in handler1.stream_events():\n",
    "                if isinstance(ev, ToolCallResult):\n",
    "                    tool_calls.append((ev.tool_name, str(ev.tool_output)))\n",
    "            \n",
    "            response1 = await handler1\n",
    "            \n",
    "            # print results\n",
    "            for i, (tool, output) in enumerate(tool_calls, 1):\n",
    "                print(f\"→ Tool Call {i}: {tool}\", flush=True)\n",
    "                print(f\"  Output: {output[:150]}...\", flush=True)\n",
    "            print(f\"\\n✅ Success: Recipe retrieved\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {str(e)[:100]}...\", flush=True)\n",
    "        \n",
    "        print(\"\\n\" + \"█\" * 80 + \"\\n\")\n",
    "    \n",
    "    # test 2: weather query\n",
    "    if remote_tools:\n",
    "        print(\"\\n\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"█ TEST 2: WEATHER QUERY (Remote FastMCP)\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"\\nQuery: what's the weather in milan?\\n\")\n",
    "        \n",
    "        try:\n",
    "            handler2 = agent.run(user_msg=\"what's the weather in milan?\")\n",
    "            \n",
    "            tool_calls = []\n",
    "            async for ev in handler2.stream_events():\n",
    "                if isinstance(ev, ToolCallResult):\n",
    "                    tool_calls.append((ev.tool_name, str(ev.tool_output)))\n",
    "            \n",
    "            response2 = await handler2\n",
    "            \n",
    "            for i, (tool, output) in enumerate(tool_calls, 1):\n",
    "                print(f\"→ Tool Call {i}: {tool}\", flush=True)\n",
    "                print(f\"  Output: {output[:150]}...\", flush=True)\n",
    "            print(f\"\\n✅ Success: Weather data retrieved\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {str(e)[:100]}...\", flush=True)\n",
    "        \n",
    "        print(\"\\n\" + \"█\" * 80 + \"\\n\")\n",
    "    \n",
    "    # test 3: video transcription\n",
    "    if docker_tools:\n",
    "        print(\"\\n\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"█ TEST 3: VIDEO TRANSCRIPTION (Docker Gateway)\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"\\nQuery: get transcript from https://www.youtube.com/watch?v=Fhy_VFMlE9s\\n\")\n",
    "        \n",
    "        try:\n",
    "            handler3 = agent.run(user_msg=\"get transcript from https://www.youtube.com/watch?v=Fhy_VFMlE9s\")\n",
    "            \n",
    "            tool_calls = []\n",
    "            async for ev in handler3.stream_events():\n",
    "                if isinstance(ev, ToolCallResult):\n",
    "                    tool_calls.append((ev.tool_name, str(ev.tool_output)))\n",
    "            \n",
    "            response3 = await handler3\n",
    "            \n",
    "            for i, (tool, output) in enumerate(tool_calls, 1):\n",
    "                print(f\"→ Tool Call {i}: {tool}\", flush=True)\n",
    "                print(f\"  Output: {output[:150]}...\", flush=True)\n",
    "            print(f\"\\n✅ Success: Transcript retrieved (length: {len(str(response3))} chars)\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {str(e)[:100]}...\", flush=True)\n",
    "        \n",
    "        print(\"\\n\" + \"█\" * 80 + \"\\n\")\n",
    "    \n",
    "    # test 4: image generation\n",
    "    if gradio_tools:\n",
    "        print(\"\\n\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"█ TEST 4: IMAGE GENERATION (Hugging Face Space)\")\n",
    "        print(\"█\" * 80)\n",
    "        print(\"\\nQuery: generate an image of a sunset over mountains\")\n",
    "        print(\"note: may fail due to quota limits\\n\")\n",
    "        \n",
    "        try:\n",
    "            handler4 = agent.run(user_msg=\"generate an image of a sunset over mountains\")\n",
    "            generated_image = None\n",
    "            has_quota_error = False\n",
    "            tool_calls = []\n",
    "            \n",
    "            async for ev in handler4.stream_events():\n",
    "                if isinstance(ev, ToolCallResult):\n",
    "                    output_str = str(ev.tool_output)\n",
    "                    tool_calls.append((ev.tool_name, output_str))\n",
    "                    \n",
    "                    # check for quota errors\n",
    "                    if 'ZeroGPU' in output_str or 'quota' in output_str.lower():\n",
    "                        has_quota_error = True\n",
    "                    \n",
    "                    # try to extract image\n",
    "                    if hasattr(ev.tool_output, 'content'):\n",
    "                        for content_item in ev.tool_output.content:\n",
    "                            if hasattr(content_item, 'type') and content_item.type == 'image':\n",
    "                                try:\n",
    "                                    image_bytes = base64.b64decode(content_item.data)\n",
    "                                    generated_image = Image.open(BytesIO(image_bytes))\n",
    "                                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                                    image_path = f\"generated_image_{timestamp}.png\"\n",
    "                                    generated_image.save(image_path)\n",
    "                                except:\n",
    "                                    pass\n",
    "            \n",
    "            response4 = await handler4\n",
    "            \n",
    "            for i, (tool, output) in enumerate(tool_calls, 1):\n",
    "                print(f\"→ Tool Call {i}: {tool}\", flush=True)\n",
    "                if 'ImageContent' in output:\n",
    "                    print(f\"  Output: [Image data]\", flush=True)\n",
    "                else:\n",
    "                    print(f\"  Output: {output[:150]}...\", flush=True)\n",
    "            \n",
    "            if generated_image:\n",
    "                print(f\"\\n✅ Success: Image generated!\", flush=True)\n",
    "                print(f\"Saved to: {image_path}\", flush=True)\n",
    "                display(generated_image)\n",
    "            elif has_quota_error:\n",
    "                print(f\"\\n⚠️ Quota limit reached - ZeroGPU quota exceeded\", flush=True)\n",
    "            else:\n",
    "                print(f\"\\n⚠️ Image generation failed or unavailable\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {str(e)[:100]}...\", flush=True)\n",
    "        \n",
    "        print(\"\\n\" + \"█\" * 80 + \"\\n\")\n",
    "    \n",
    "    # summary\n",
    "    print(\"\\n\")\n",
    "    print(\"█\" * 80)\n",
    "    print(\"█ HYBRID AGENT DEMO COMPLETE\")\n",
    "    print(\"█\" * 80)\n",
    "    successful_sources = len([t for t in [local_tools, remote_tools, docker_tools, gradio_tools] if t])\n",
    "    print(f\"\\n✅ Connected to {successful_sources}/4 MCP sources\")\n",
    "    print(f\"✅ Total tools available: {len(all_tools)}\")\n",
    "    print(\"\\n\" + \"█\" * 80)\n",
    "\n",
    "# run the example\n",
    "await hybrid_agent_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. the problem: tool overload\n",
    "\n",
    "### challenge\n",
    "as we add more mcp servers and tools, we face a problem:\n",
    "- **context window limitations**: too many tools overwhelm the llm's context\n",
    "- **tool selection confusion**: the agent may not call the right tools\n",
    "- **performance degradation**: more tools = slower reasoning\n",
    "\n",
    "### example scenario:\n",
    "imagine an agent with 50+ tools from 10 different mcp servers:\n",
    "- recipes (5 tools)\n",
    "- weather (3 tools)\n",
    "- youtube (2 tools)\n",
    "- image generation (4 tools)\n",
    "- database queries (10 tools)\n",
    "- file operations (8 tools)\n",
    "- web scraping (6 tools)\n",
    "- email (4 tools)\n",
    "- calendar (5 tools)\n",
    "- analytics (8 tools)\n",
    "\n",
    "**result**: the agent becomes confused and may:\n",
    "- call the wrong tools\n",
    "- miss relevant tools\n",
    "- take longer to reason\n",
    "- produce incorrect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. solution: multi-agent triage system\n",
    "\n",
    "### architecture\n",
    "instead of one agent with all tools, we create:\n",
    "1. **triage agent**: routes requests to specialist agents (no tools)\n",
    "2. **specialist agents**: each has a focused set of tools\n",
    "   - recipeagent: local mcp tools (recipes)\n",
    "   - weatheragent: remote mcp tools (weather)\n",
    "   - videoagent: docker gateway tools (youtube)\n",
    "   - imageagent: hugging face tools (image generation)\n",
    "\n",
    "### benefits:\n",
    "- **focused context**: each specialist only sees relevant tools\n",
    "- **better tool selection**: specialists are experts in their domain\n",
    "- **scalability**: easy to add new specialists without overloading existing ones\n",
    "- **maintainability**: each agent can be updated independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multiagent_triage_system():\n",
    "    \"\"\"implement a multi-agent triage system with specialized agents\"\"\"\n",
    "    \n",
    "    print(\"=== setting up multi-agent triage system ===\")\n",
    "    \n",
    "    # connect to all mcp sources\n",
    "    docker_client = BasicMCPClient(\n",
    "        \"http://localhost:8080/mcp\",\n",
    "        headers={\"Authorization\": f\"Bearer {DOCKER_GATEWAY_TOKEN}\"}\n",
    "    ) if DOCKER_GATEWAY_TOKEN else None\n",
    "    remote_client = BasicMCPClient(\"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\")\n",
    "    local_client = BasicMCPClient(\"http://localhost:8000/mcp\")\n",
    "    gradio_client = BasicMCPClient(\"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\")\n",
    "    \n",
    "    # get tools from each source\n",
    "    print(\"\\nloading tools...\")\n",
    "    \n",
    "    # docker tools\n",
    "    if docker_client:\n",
    "        try:\n",
    "            docker_tools = await aget_tools_from_mcp_url(\n",
    "                \"http://localhost:8080/mcp\",\n",
    "                client=docker_client,\n",
    "                allowed_tools=[\"get_transcript\"],\n",
    "            )\n",
    "            print(f\"  docker gateway: {len(docker_tools)} tools\")\n",
    "        except Exception as e:\n",
    "            print(f\"  docker gateway: error ({e})\")\n",
    "            docker_tools = []\n",
    "    else:\n",
    "        docker_tools = []\n",
    "        print(\"  docker gateway: skipped (no token)\")\n",
    "    \n",
    "    # local tools\n",
    "    try:\n",
    "        local_tools = await aget_tools_from_mcp_url(\n",
    "            \"http://localhost:8000/mcp\",\n",
    "            client=local_client,\n",
    "            allowed_tools=[\"list_recipes\", \"get_recipe_instructions\"],\n",
    "        )\n",
    "        print(f\"  local mcp: {len(local_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  local mcp: error ({e})\")\n",
    "        local_tools = []\n",
    "    \n",
    "    # remote tools\n",
    "    try:\n",
    "        remote_tools = await aget_tools_from_mcp_url(\n",
    "            \"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\",\n",
    "            client=remote_client,\n",
    "            allowed_tools=[\"get_weather\"],\n",
    "        )\n",
    "        print(f\"  remote fastmcp: {len(remote_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  remote fastmcp: error ({e})\")\n",
    "        remote_tools = []\n",
    "    \n",
    "    # gradio tools\n",
    "    try:\n",
    "        gradio_tools = await aget_tools_from_mcp_url(\n",
    "            \"https://hysts-mcp-flux-1-schnell.hf.space/gradio_api/mcp/\",\n",
    "            client=gradio_client,\n",
    "        )\n",
    "        print(f\"  hugging face: {len(gradio_tools)} tools\")\n",
    "    except Exception as e:\n",
    "        print(f\"  hugging face: unavailable ({e})\")\n",
    "        gradio_tools = []\n",
    "    \n",
    "    # initialize llm\n",
    "    llm = OpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # create specialized agents\n",
    "    print(\"\\ncreating specialized agents...\")\n",
    "    \n",
    "    recipe_agent = ReActAgent(\n",
    "        name=\"RecipeAgent\",\n",
    "        description=\"handles recipe queries and cooking instructions\",\n",
    "        tools=local_tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "            \"you are a recipe expert with access to italian recipes. \"\n",
    "            \"use your tools to provide detailed cooking instructions.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    weather_agent = ReActAgent(\n",
    "        name=\"WeatherAgent\",\n",
    "        description=\"provides weather information for any location\",\n",
    "        tools=remote_tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "            \"you are a weather assistant. use your weather tool to provide \"\n",
    "            \"current weather information for any location.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    video_agent = ReActAgent(\n",
    "        name=\"VideoAgent\",\n",
    "        description=\"transcribes youtube videos given the url\",\n",
    "        tools=docker_tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "            \"you are a video transcription specialist. you can get transcripts \"\n",
    "            \"from youtube videos using your tools.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    image_agent = ReActAgent(\n",
    "        name=\"ImageAgent\",\n",
    "        description=\"generates images from text prompts using flux model\",\n",
    "        tools=gradio_tools,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "            \"you are an image generation specialist using the flux model. \"\n",
    "            \"create images from text descriptions using your tools.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # triage agent: routes to appropriate specialist\n",
    "    triage_agent = ReActAgent(\n",
    "        name=\"TriageAgent\",\n",
    "        description=\"routes user requests to the appropriate specialist agent\",\n",
    "        tools=[],  # no tools, only routes\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "            \"you are a routing agent. you must immediately hand off EVERY request to a specialist agent.\\n\"\n",
    "            \"you have NO tools to answer questions - you can ONLY use handoff.\\n\\n\"\n",
    "            \"routing rules:\\n\"\n",
    "            \"- recipes/cooking → hand off to RecipeAgent\\n\"\n",
    "            \"- weather queries → hand off to WeatherAgent\\n\"\n",
    "            \"- youtube videos → hand off to VideoAgent\\n\"\n",
    "            \"- image generation → hand off to ImageAgent\\n\\n\"\n",
    "            \"CRITICAL: use the handoff tool, never call agent names directly.\"\n",
    "        ),\n",
    "        can_handoff_to=[\"RecipeAgent\", \"WeatherAgent\", \"VideoAgent\", \"ImageAgent\"],\n",
    "    )\n",
    "    \n",
    "    # wire agents together in workflow\n",
    "    print(\"\\ncreating agent workflow...\")\n",
    "    agent_workflow = AgentWorkflow(\n",
    "        agents=[triage_agent, recipe_agent, weather_agent, video_agent, image_agent],\n",
    "        root_agent=triage_agent.name,\n",
    "        initial_state={},\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== multi-agent system ready! ===\")\n",
    "    print(\"\\nagents:\")\n",
    "    print(\"  - TriageAgent (router)\")\n",
    "    print(f\"  - RecipeAgent (local mcp) - {len(local_tools)} tools\")\n",
    "    print(f\"  - WeatherAgent (remote mcp) - {len(remote_tools)} tools\")\n",
    "    print(f\"  - VideoAgent (docker gateway) - {len(docker_tools)} tools\")\n",
    "    print(f\"  - ImageAgent (hugging face) - {len(gradio_tools)} tools\")\n",
    "    \n",
    "    # test the multi-agent system\n",
    "    if local_tools:\n",
    "        print(\"\\n=== test 1: recipe query ===\")\n",
    "        resp1 = await agent_workflow.run(user_msg=\"how to make tiramisu?\")\n",
    "        print(f\"\\nresponse: {resp1}\")\n",
    "    \n",
    "    if remote_tools:\n",
    "        print(\"\\n=== test 2: weather query ===\")\n",
    "        resp2 = await agent_workflow.run(user_msg=\"what's the weather in rome?\")\n",
    "        print(f\"\\nresponse: {resp2}\")\n",
    "    \n",
    "    if docker_tools:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TEST 3: VIDEO TRANSCRIPTION (Multi-Agent Triage)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"User Query: get transcript from https://www.youtube.com/watch?v=Fhy_VFMlE9s\")\n",
    "        print(\"Expected Flow: TriageAgent -> VideoAgent\\n\")\n",
    "        \n",
    "        resp3 = await agent_workflow.run(user_msg=\"get transcript from https://www.youtube.com/watch?v=Fhy_VFMlE9s\")\n",
    "        print(f\"\\nFinal Response (truncated): {str(resp3)[:500]}...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "# run the multi-agent system\n",
    "await multiagent_triage_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. gradio ui for multi-agent system\n",
    "\n",
    "### creating a user-friendly interface\n",
    "\n",
    "the `mcp_client_gradio.py` file provides a web interface for the multi-agent system:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    chat,\n",
    "    type=\"messages\",\n",
    "    title=\"🤖 MCP Multiagent Triage Chatbot\",\n",
    "    description=(\n",
    "        \"chat with specialized mcp agents:\\n\\n\"\n",
    "        \"🍕 **RecipeAgent** - get recipes and cooking instructions\\n\"\n",
    "        \"🌤️ **WeatherAgent** - check weather for any location\\n\"\n",
    "        \"🎥 **VideoAgent** - transcribe youtube videos\\n\"\n",
    "        \"🎨 **ImageAgent** - generate images using flux model\\n\\n\"\n",
    "        \"the TriageAgent will automatically route your request!\"\n",
    "    ),\n",
    "    save_history=True,  # persistent chat history\n",
    ")\n",
    "\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "### to run the gradio interface:\n",
    "\n",
    "```bash\n",
    "python mcp_client_gradio.py\n",
    "```\n",
    "\n",
    "### features:\n",
    "- **persistent history**: conversations saved in browser\n",
    "- **streaming responses**: see agent reasoning in real-time\n",
    "- **tool call visibility**: see which tools are being called\n",
    "- **example prompts**: quick start with pre-defined queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. integrating mcp in other platforms\n",
    "\n",
    "### cursor ide\n",
    "add to cursor settings (`.cursor/settings.json`):\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"recipes\": {\n",
    "      \"url\": \"http://localhost:8000/mcp\"\n",
    "    },\n",
    "    \"weather\": {\n",
    "      \"url\": \"https://unnecessary-crimson-wildebeest.fastmcp.app/mcp\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### claude desktop\n",
    "add to claude config (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"recipes\": {\n",
    "      \"url\": \"http://localhost:8000/mcp\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### chatgpt\n",
    "use the \"actions\" feature to add mcp servers:\n",
    "1. go to chatgpt settings → actions\n",
    "2. add new action with mcp server url\n",
    "3. configure authentication if needed\n",
    "\n",
    "### lm studio\n",
    "add to lm studio mcp config (similar to cursor format):\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"recipes\": {\n",
    "      \"url\": \"http://localhost:8000/mcp\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. summary and best practices\n",
    "\n",
    "### key takeaways\n",
    "\n",
    "1. **mcp provides unified tool access**\n",
    "   - tools, resources, and prompts in one protocol\n",
    "   - works across different platforms and languages\n",
    "\n",
    "2. **multiple hosting options**\n",
    "   - local: full control, requires hosting\n",
    "   - docker gateway: easy integration with docker ecosystem (requires bearer token)\n",
    "   - fastmcp cloud: free hosting, automatic deployments\n",
    "   - hugging face spaces: 200+ ready-to-use servers\n",
    "\n",
    "3. **debugging with inspector**\n",
    "   - test tools before integration\n",
    "   - explore resources and prompts\n",
    "   - view logs and debug issues\n",
    "\n",
    "4. **multi-agent architecture for scale**\n",
    "   - avoid tool overload with specialized agents\n",
    "   - triage pattern for intelligent routing\n",
    "   - better performance and maintainability\n",
    "\n",
    "### best practices\n",
    "\n",
    "1. **start small**: begin with 1-2 mcp servers\n",
    "2. **use inspector**: always test tools before production\n",
    "3. **specialize agents**: keep each agent focused on specific tasks\n",
    "4. **monitor performance**: watch for tool selection issues\n",
    "5. **document tools**: clear descriptions help llms choose correctly\n",
    "6. **version control**: track mcp server changes\n",
    "7. **error handling**: gracefully handle mcp server failures\n",
    "8. **authentication**: always use bearer tokens for docker gateway\n",
    "\n",
    "### next steps\n",
    "\n",
    "1. create your own mcp server for your domain\n",
    "2. explore hugging face spaces for additional capabilities\n",
    "3. implement multi-agent systems for complex workflows\n",
    "4. integrate mcp into your existing applications\n",
    "5. contribute to the mcp ecosystem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appendix: useful resources\n",
    "\n",
    "### documentation\n",
    "- mcp specification: https://modelcontextprotocol.io/\n",
    "- fastmcp docs: https://github.com/jlowin/fastmcp\n",
    "- llamaindex mcp: https://docs.llamaindex.ai/en/stable/examples/tools/mcp/\n",
    "\n",
    "### repositories\n",
    "- mcp python sdk: https://github.com/modelcontextprotocol/python-sdk\n",
    "- docker mcp gateway: https://github.com/docker/mcp-gateway\n",
    "- fastmcp quickstart: https://github.com/PrefectHQ/fastmcp-quickstart-template\n",
    "\n",
    "### hosting\n",
    "- fastmcp cloud: https://fastmcp.cloud/\n",
    "- hugging face spaces: https://huggingface.co/spaces\n",
    "\n",
    "### community\n",
    "- mcp discord: https://discord.gg/modelcontextprotocol\n",
    "- github discussions: https://github.com/modelcontextprotocol/specification/discussions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCP-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
